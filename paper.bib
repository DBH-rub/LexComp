@inproceedings{albright2007natural,
  title={Natural classes are not enough: Biased generalization in novel onset clusters},
  author={Albright, Adam},
  booktitle={15th Manchester Phonology Meeting, Manchester, UK},
  pages={24--26},
  year={2007}
}


@misc{corpusTools_phonoalign,
  author       = "Hall, Kathleen Currie and Mackie, J. Scott and Lo, Roger Yu-Hsiang",
  title        = {CorpusTools: phono\_align.py},
  year         = {2025},
  url          = {https://github.com/PhonologicalCorpusTools/CorpusTools/blob/master/corpustools/symbolsim/phono_align.py},
  note         = {Accessed: 2025-05-16}
}

@article{Hall2019_PCT,
   author = "Hall, Kathleen Currie and Mackie, J. Scott and Lo, Roger Yu-Hsiang",
   title = "Phonological CorpusTools", 
   journal= "International Journal of Corpus Linguistics",
   year = "2019",
   volume = "24",
   number = "4",
   pages = "522-535",
   doi = "https://doi.org/10.1075/ijcl.18009.hal",
   url = "https://www.jbe-platform.com/content/journals/10.1075/ijcl.18009.hal",
   publisher = "John Benjamins",
   issn = "1384-6655",
   type = "Journal Article"
  }

@article{Arnold2017,
    doi = {10.1371/journal.pone.0174623},
    author = {Arnold, Denis AND Tomaschek, Fabian AND Sering, Konstantin AND Lopez, Florence AND Baayen, R. Harald},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Words from spontaneous conversational speech can be recognized with human-like accuracy by an error-driven learning algorithm that discriminates between meanings straight from smart acoustic features, bypassing the phoneme as recognition unit},
    year = {2017},
    month = {04},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0174623},
    pages = {1-16},
    abstract = {Sound units play a pivotal role in cognitive models of auditory comprehension. The general consensus is that during perception listeners break down speech into auditory words and subsequently phones. Indeed, cognitive speech recognition is typically taken to be computationally intractable without phones. Here we present a computational model trained on 20 hours of conversational speech that recognizes word meanings within the range of human performance (model 25%, native speakers 20–44%), without making use of phone or word form representations. Our model also generates successfully predictions about the speed and accuracy of human auditory comprehension. At the heart of the model is a ‘wide’ yet sparse two-layer artificial neural network with some hundred thousand input units representing summaries of changes in acoustic frequency bands, and proxies for lexical meanings as output units. We believe that our model holds promise for resolving longstanding theoretical problems surrounding the notion of the phone in linguistic theory.},
    number = {4},

}
@misc{ChuangBaayen2021DiscriminativeLearningandtheLexicon,
      author = "Yu-Ying Chuang and R. Harald Baayen",
      title = "Discriminative Learning and the Lexicon: NDL and LDL",
      year = "2021",
      month = "12",
      publisher = "Oxford University Press",
      doi = "10.1093/acrefore/9780199384655.013.375",
      url = "https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-375"
}

@article{Milin2017,
    doi = {10.1371/journal.pone.0171935},
    author = {Milin, Petar AND Feldman, Laurie Beth AND Ramscar, Michael AND Hendrix, Peter AND Baayen, R. Harald},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Discrimination in lexical decision},
    year = {2017},
    month = {02},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0171935},
    pages = {1-42},
    abstract = {In this study we present a novel set of discrimination-based indicators of language processing derived from Naive Discriminative Learning (ndl) theory. We compare the effectiveness of these new measures with classical lexical-distributional measures—in particular, frequency counts and form similarity measures—to predict lexical decision latencies when a complete morphological segmentation of masked primes is or is not possible. Data derive from a re-analysis of a large subset of decision latencies from the English Lexicon Project, as well as from the results of two new masked priming studies. Results demonstrate the superiority of discrimination-based predictors over lexical-distributional predictors alone, across both the simple and primed lexical decision tasks. Comparable priming after masked corner and cornea type primes, across two experiments, fails to support early obligatory segmentation into morphemes as predicted by the morpho-orthographic account of reading. Results fit well with ndl theory, which, in conformity with Word and Paradigm theory, rejects the morpheme as a relevant unit of analysis. Furthermore, results indicate that readers with greater spelling proficiency and larger vocabularies make better use of orthographic priors and handle lexical competition more efficiently.},
    number = {2},

}

@article{Leys2013,
title = {Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median},
journal = {Journal of Experimental Social Psychology},
volume = {49},
number = {4},
pages = {764-766},
year = {2013},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2013.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0022103113000668},
author = {Christophe Leys and Christophe Ley and Olivier Klein and Philippe Bernard and Laurent Licata},
keywords = {Median absolute deviation, Outlier, MAD},
abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software.}
}

@inbook{Howell2005,
author = {Howell, David C.},
publisher = {John Wiley & Sons, Ltd},
isbn = {9780470013199},
title = {Median Absolute Deviation},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
chapter = {},
pages = {},
doi = {https://doi.org/10.1002/0470013192.bsa384},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470013192.bsa384},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470013192.bsa384},
year = {2005},
keywords = {dispersion, efficiency, estimation, estimators, scale, variability},
abstract = {Abstract The median deviation is a measure of scale based on the median of the absolute deviations from the median of the distribution. For a normal distribution, the median deviation is somewhat less efficient than the standard deviation or the average deviation as a measure of scale, but this advantage quickly reverses for distributions with heavier tails.}
}


@book{phoible,
  address   = {Jena},
  editor    = {Steven Moran and Daniel McCloy},
  publisher = {Max Planck Institute for the Science of Human History},
  title     = {PHOIBLE 2.0},
  url       = {https://phoible.org/},
  year      = {2019}
}

@techreport{mfa_english_us_mfa_dictionary_2023,
    author={McAuliffe, Michael and Sonderegger, Morgan},
    title={English (US) MFA dictionary v2.2.1},
    address={\url{https://mfa-models.readthedocs.io/pronunciation dictionary/English/English (US) MFA dictionary v2_2_1.html}},
    year={2023},
    month={May},
}

@Manual{hunspell2023,
    title = {hunspell: High-Performance Stemmer, Tokenizer, and Spell Checker},
    author = {Jeroen Ooms},
    year = {2023},
    note = {R package version 3.0.3},
    url = {https://CRAN.R-project.org/package=hunspell},
  }

@Article{Brysbaert2009,
  Title                    = {{{M}oving beyond {K}u{\v{c}}era and {F}rancis: a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for {A}merican {E}nglish}},
  Author                   = {Brysbaert, Marc and New, Boris},
  Journal                  = {Behavior {R}esearch {M}ethods},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {977--990},
  Volume                   = {41},

  Doi                      = {10.3758/BRM.41.4.977}
}

@Article{TangBH2023,
  author={Tang, Kevin and Baer-Henney, Dinah},
  title   = {Modelling {L1} and the artificial language during artificial language learning},
  journal = {Laboratory Phonology},
  year    = {2023},
  volume  = {14},
  number  = {1},
  pages   = {1--54},
  doi     = {10.16995/labphon.6460},
}

@article{baayen2011amorphous,
  title={An amorphous model for morphological processing in visual comprehension based on naive discriminative learning.},
  author={Baayen, R Harald and Milin, Petar and {\DJ}ur{\dj}evi{\'c}, Dusica Filipovi{\'c} and Hendrix, Peter and Marelli, Marco},
  journal={Psychological Review},
  volume={118},
  number={3},
  pages={438},
  year={2011},
  publisher={American Psychological Association}
}

@manual{arppe2015ndl,
    title = {ndl: Naive Discriminative Learning},
    author = {{Arppe, A.} and {Hendrix, P.} and {Milin, P.} and {Baayen, R. H.} and {Sering, T.} and {Shaoul, C.}},
    year = {2018},
    note = {R package version 0.2.18},
    url = {https://CRAN.R-project.org/package=ndl},
  }


@Article{Brysbaert2012,
author={Brysbaert, Marc
and New, Boris
and Keuleers, Emmanuel},
title={Adding part-of-speech information to the {SUBTLEX-US} word frequencies},
journal={Behavior Research Methods},
year={2012},
month={Dec},
day={01},
volume={44},
number={4},
pages={991-997},
abstract={The SUBTLEX-US corpus has been parsed with the CLAWS tagger, so that researchers have information about the possible word classes (parts‐of‐speech, or PoSs) of the entries. Five new columns have been added to the SUBTLEX-US word frequency list: the dominant (most frequent) PoS for the entry, the frequency of the dominant PoS, the frequency of the dominant PoS relative to the entry's total frequency, all PoSs observed for the entry, and the respective frequencies of these PoSs. Because the current definition of lemma frequency does not seem to provide word recognition researchers with useful information (as illustrated by a comparison of the lemma frequencies and the word form frequencies from the Corpus of Contemporary American English), we have not provided a column with this variable. Instead, we hope that the full list of PoS frequencies will help researchers to collectively determine which combination of frequencies is the most informative.},
issn={1554-3528},
doi={10.3758/s13428-012-0190-4},
url={https://doi.org/10.3758/s13428-012-0190-4}
}


@article{bailey2001determinants,
  title={Determinants of wordlikeness: Phonotactics or lexical neighborhoods?},
  author={Bailey, Todd M. and Hahn, Ulrike},
  journal={Journal of Memory and Language},
  volume={44},
  number={4},
  pages={568--591},
  year={2001},
  doi = {10.1006/jmla.2000.2756},
  publisher={Elsevier}
}



@Manual{RCT2013,
  Title                    = {R: A Language and Environment for Statistical Computing},

  Address                  = {Vienna, Austria},
  Author                   = {{R Core Team}},
  Organization             = {R Foundation for Statistical Computing},
  Year                     = {2013},

  Url                      = {http://www.R-project.org/}
}

@Article{Nosofsky1986,
  author    = {Nosofsky, Robert M.},
  title     = {Attention, similarity, and the identification-categorization relationship.},
  journal   = {Journal of Experimental Psychology: General},
  year      = {1986},
  volume    = {115},
  number    = {1},
  pages     = {39-57},
  address   = {US},
  doi       = {10.1037/0096-3445.115.1.39},
  keywords  = {*Classification (Cognitive Process), *Selective Attention, *Stimulus Similarity, Models},
  publisher = {American Psychological Association},
}



@misc{LexComp,
  author = {D. Baer-Henney and K. Tang},
  title = {LexComp:R-package for comparing lexicons through estimating wordlikeness with discriminative and analogical learning},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/DBH-rub/LexComp/}
}

